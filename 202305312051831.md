## Pytorch中神经网络模块化接口nn

torch.nn：专门为神经网络设计的模块化接口。nn构建于autograd之上，可以用来定义和运行神经网络

##### nn.Module：

nn中十分重要的类，包括网络各层的定义及forward方法

如何使用nn定义自己的神经网络：（以LeNet网络为列）

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

class LeNet(nn.Module):									#需要继承nn.Module类，并在子类中实现forward方法
    def __init__(self):									#一般把网络中具有可学习参数的层放在构造函数__init__()中，如卷积层、全连接层
        super(LeNet, self).__init__()					#不具有可学习参数的层可放在构造函数中，也可放在forward函数中，但在forward中需要用                                                             nn.functional.xxx来调用相关层，如：nn.functional.relu()
        self.conv1 = nn.Conv2d(1, 6, (5, 5))
        self.conv2 = nn.Conv2d(6, 16, (5, 5))
        self.fc1 = nn.Linear(256, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
     
    def forward(self, x):								 #在子类中必须定义forward函数，用于保证神经网络的正向传播，同时backward函数也因此会被                                                           自动实现（利用autograd）,注意：forward函数通过各种形式把__init__()中的结构包括到															  函数当中来
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))  
        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))
        x = x.view(x.size()[0], -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        return x
if __name__ == '__main__':
    net = LeNet()
    input_image = torch.FloatTensor(10, 1, 28, 28)
    output = net(input_image)							#forward函数的一个重要作用，就是使得在进行网络训练时，在调用LeNet中方法时不再需要通过：															实例名+方法名(x)的方式调用网络结构(如：net.forward(x))，而是可以直接通过：实例名															  (x)的方式直接调用网络结构

```

```
Tips：	关于super(LeNet, self).__init__():在子类LeNet中如果重写__init__()函数，那父类nn.Module的构造函数信息就会被覆盖，为了使其不被覆盖，就需要使用super来继续继承父类的构造函数信息
```

```python
#Pytorch深度神经网络大体框架
class Module(nn.Module):
    def __init__(self):
        super().__init__()
        # ......

    def forward(self, x):
        # ......
        return x


data = ......  # 输入数据

# 实例化一个对象
model = Module()

# 前向传播
model(data)
```

## Pytorch功能包

- ```
  torchsummary——神经网络结果可视化功能包
  ```

1. 导入：

   ```python
   from torchsummary import summary
   ```

2. 可视化模型：

   ```python
   summary(model, input_size, batch_size, device)
   ```

​		_model_:实例化的模型名

​		_input_size_：输入尺寸

​		_batch_size_：批次大小

​		_device_：运行设备编号





## Pytorch自制数据集



## torch.nn相关语法简记

- ```python
  模型容器Containers：
  ```

![img](https://raw.githubusercontent.com/WZY12136/Picture/main/202302031542226.png)

![img](https://raw.githubusercontent.com/WZY12136/Picture/main/202302031543813.png)



- ```python
  torch.nn.ReLu(inplace=True)
  ```

作用：选择是否进行覆盖运算，True表示覆盖上次的计算值

- ```python
  torch.nn.xxx与torch.nn.functional.xxx的区别
  ```

两者本质上无别，都是对相关网络层等的调用，只是在__forward函数__中常用torch.nn.functional.xxx，而在构造函数_\_init__()中常用torch.nn.xxx

- ```python
  torch.nn.functional.pad(x, pad, mode, value)
  ```

函数作用：对feature map进行padding操作

1. x：需进行padding的特征图向量
2. pad：指明需要进行padding的方向（左   右   上   下），一般是（1, 1, 1, 1）对左右上下都进行padding，如果某一方向不需要进行padding，则将其值改为0即可

![image-20230204204352505](https://raw.githubusercontent.com/WZY12136/Picture/main/202302042043536.png)

3. mode:填充的方式，一般为：constant

4. value：填充的具体数值

- ```python
  torch.nn.DataParallel(module, device_ids=None, output_device=None)
  ```

使用多GPU进行训练

·参数：

_module_：实例化的神经网络模型

_device_ids_：对应的GPU标号

_output_device_：输入结果的CPU标号

- ```python
  nn.BatchNorm3d(num_features, eps=1e^05, momentum=0.1, affine=True)
  ```

  ###### BN层计算式：

  $$
  y = \frac{x-E(X)}{\sqrt\sigma(x)+\varepsilon}*\nu+\beta
  $$

  其中$\nu$和$\beta$是BN层的可学习参数，如果$affine=True$，则两个参数可学习，$affine=False$则不可学习
